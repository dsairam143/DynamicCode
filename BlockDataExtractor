
import lxml.html
import requests
import re
from datetime import datetime
from operator import itemgetter
from lxml.html import tostring
from tabulate import tabulate
from bs4 import BeautifulSoup
import pandas as pd
import dateutil.parser as dparser
resp = requests.get('https://www.ndtv.com/india').content
root = lxml.html.fromstring(resp)
title = root.xpath('//*[@id="ins_storylist"]/ul')
html_element = tostring(title[0])
divs = BeautifulSoup(html_element).find('body').findNext()
tableHeaders = ['Extract Date','Heading', 'PContent', 'Content', 'Url', 'ImageUrl', 'PublishedDate']
presentDate = datetime.now().date()
tableData = [tableHeaders]
for div in divs.findChildren(recursive=False):
    try:
        MainHeading = None
        Content = None
        ImageLink = None
        Url = None
        pContent = None
        published_date = None
        headingList = [h.text.strip() for h in div.find_all(re.compile('h2|h3|h4'))]
        pList = [[len(p), p.text.strip()] for p in div.find_all('p')]

        divList = []
        for div_e in div.find_all('div'):
            if len(div_e.findChildren(recursive=False)) == 0:
                divList.append([len(div_e.text.strip()), div_e.text])
            else:
                for div_e in div_e.findChildren(recursive=False):
                    divList.append([len(div_e.text.strip()), div_e.text])

        imgList = [img['src'] for img in div.find_all('img')]
        aList = [[a.text.strip(), a['href']] for a in div.find_all('a', attrs={'href':re.compile('.+')})]
        if len(divList) == 0:
            divList = [[len(div.text.strip()), div.text] for div in div.find_all('span')]
        published_date = re.search('[a-zA-Z\.]* \d?\d, \d{4}|'
                                   '\d{4}[-\.]\d\d[-\.]\d\d|'
                                   '\d\d?[-\/.]\d\d?[-\/.]\d{4}|'
                                   '[a-zA-Z]* \d\d?, [a-zA-Z]* \d{4}|'
                                   '\d\d? [a-zA-Z]*  ?\d{4}|'
                                   '\d\d?[a-zA-Z]+ [a-zA-Z]+, \d{4}', div.text)

        try:
            published_date = dparser.parse(published_date.group(0), fuzzy=True).date()
        except:
            published_date = None
        if len(headingList) != 0:
            MainHeading = headingList[0]
        elif len(aList) != 0:
            anch = [a[0] for a in aList if len(a[0])>5]
            if len(anch) != 0:
                MainHeading = anch[0]
            else:
                continue
        else:
            continue

        if len(pList) !=0:
            pList = sorted(pList, key=itemgetter(0), reverse=True)
            pContent = re.sub(' +',' ', pList[0][1].replace('\n', ''))
        if len(divList) != 0:
            divList = sorted(divList, key=itemgetter(0), reverse=True)
            Content = divList[0][1].replace('\n', '')
        if len(imgList) != 0:
            ImageLink = imgList[0]
        if len(aList) != 0:
            anch = [a[1] for a in aList]
            Url = anch[0]

        tableData.append([presentDate, MainHeading, pContent, Content, Url, ImageLink, published_date])
    except Exception as e:
        print(e)

    # print('------------------------------------------------------------------')
print(tabulate(tableData))
df = pd.DataFrame(tableData[1:], columns=tableHeaders)
print(len(df['PContent']))
