import lxml.etree
import lxml.html
import requests
import re
from datetime import datetime
from operator import itemgetter
from lxml.html import fromstring, tostring
from tabulate import tabulate
from bs4 import BeautifulSoup
import dateutil.parser as dparser
from io import StringIO
resp = requests.get('http://www.ratingdada.com/1/telugu-movie-reviews-ratings').content
root = lxml.html.fromstring(resp)
title = root.xpath('//*[@id="titlesdiv"]')
# print(title)
html_element = tostring(title[0])
divs = BeautifulSoup(html_element).find(re.compile('div|ul'))
print(divs)
tableHeaders = ['Extract Date','Heading', 'PContent', 'Content', 'Url', 'ImageUrl', 'PublishedDate']
presentDate = datetime.now().date()
tableData = [tableHeaders]
for div in divs.findChildren(recursive=False):

    try:
        MainHeading = None
        Content = None
        ImageLink = None
        Url = None
        pContent = None
        published_date = None
        headingList = [h.text.strip() for h in div.find_all(re.compile('h2|h3|h4'))]
        pList = [[len(p), p.text.strip()] for p in div.find_all('p')]
        divList = [[len(div.text.strip()), div.text] for div_e in div.find_all('div') for div in div_e.findChildren(recursive = False) ]
        imgList = [img['src'] for img in div.find_all('img')]
        aList = [[a.text.strip(), a['href']] for a in div.find_all('a', attrs={'href':re.compile('.+')})]

        published_date = re.search('[a-zA-Z\.]* \d?\d, \d{4}|'
                                   '\d{4}[-\.]\d\d[-\.]\d\d|'
                                   '\d\d?[-\/.]\d\d?[-\/.]\d{4}|'
                                   '[a-zA-Z]* \d\d?, [a-zA-Z]* \d{4}|'
                                   '\d\d? [a-zA-Z]*  ?\d{4}|'
                                   '\d\d?[a-zA-Z]+ [a-zA-Z]+, \d{4}', div.text)

        try:
            published_date = dparser.parse(published_date.group(0), fuzzy=True).date()
        except:
            published_date = None
        if len(headingList) != 0:
            MainHeading = headingList[0]
        elif len(aList) != 0:
            anch = [a[0] for a in aList if len(a[0])>5]
            if len(anch) != 0:
                MainHeading = anch[0]
            else:
                continue
        else:
            continue

        if len(pList) !=0:
            pList = sorted(pList, key=itemgetter(0), reverse=True)
            pContent = re.sub(' +',' ', pList[0][1].replace('\n', ''))
        if len(divList) != 0:
            divList = sorted(divList, key=itemgetter(0), reverse=True)
            Content = divList[0][1].replace('\n', '')
        if len(imgList) != 0:
            ImageLink = imgList[0]
        if len(aList) != 0:
            anch = [a[1] for a in aList]
            Url = anch[0]

        tableData.append([presentDate, MainHeading, pContent, Content, Url, ImageLink, published_date])
    except Exception as e:
        print(e)
    # print('------------------------------------------------------------------')
print(tabulate(tableData))
# print(jsoup)
# print("My blog title is: '{}'".format(title[0].html))
